{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22d7410d",
   "metadata": {},
   "source": [
    "# Representación de Texto para Clasificación  \n",
    "## Bag-of-Words, TF-IDF y Embeddings (con Word2Vec)\n",
    "\n",
    "Este notebook explica, paso a paso, tres formas clásicas de representar texto para tareas de *machine learning*:\n",
    "\n",
    "1. **Bag-of-Words (BoW)**  \n",
    "2. **TF-IDF (Term Frequency – Inverse Document Frequency)**  \n",
    "3. **Embeddings** (y el rol de **Word2Vec**)\n",
    "\n",
    "La idea es que el código te sirva para **experimentar**, mientras que el texto en Markdown te guía con la intuición teórica, igual que si fuera una clase teórica + práctica.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2d977f",
   "metadata": {},
   "source": [
    "## 1. Bag-of-Words (BoW)\n",
    "\n",
    "### 1.1. ¿Qué es Bag-of-Words?\n",
    "\n",
    "**Idea central:** representar un documento como un **histograma de palabras**.\n",
    "\n",
    "Pasos conceptuales:\n",
    "\n",
    "1. Tomamos un documento de texto (por ejemplo: `\"el perro muerde al gato\"`).  \n",
    "2. Lo **tokenizamos** (lo separamos en palabras):  \n",
    "   `[\"el\", \"perro\", \"muerde\", \"al\", \"gato\"]`  \n",
    "3. Construimos un **vocabulario** a partir de todos los documentos del corpus:  \n",
    "   por ejemplo, con 3 documentos:\n",
    "\n",
    "   - Doc 1: `\"el perro muerde\"`  \n",
    "   - Doc 2: `\"el gato duerme\"`  \n",
    "   - Doc 3: `\"el perro duerme con el gato\"`  \n",
    "\n",
    "   El vocabulario podría ser:  \n",
    "   `[\"el\", \"perro\", \"muerde\", \"gato\", \"duerme\", \"con\"]`\n",
    "\n",
    "4. Cada documento se convierte en un **vector de enteros** cuya longitud es el tamaño del vocabulario.  \n",
    "   - La posición *i* corresponde a la palabra *i* del vocabulario.  \n",
    "   - El valor es cuántas veces aparece esa palabra en el documento (TF: *term frequency*).\n",
    "\n",
    "### 1.2. Punto clave\n",
    "\n",
    "> El tamaño del vector **NO** es el número de palabras del documento,  \n",
    "> **es el tamaño del vocabulario del corpus**.\n",
    "\n",
    "Ejemplo de conteos (BoW):\n",
    "\n",
    "- Doc 1: `\"el perro muerde\"`  \n",
    "  - `el`: 1, `perro`: 1, `muerde`: 1, resto: 0  \n",
    "  - Vector: `[1, 1, 1, 0, 0, 0]`\n",
    "\n",
    "- Doc 2: `\"el gato duerme\"`  \n",
    "  - `el`: 1, `gato`: 1, `duerme`: 1, resto: 0  \n",
    "  - Vector: `[1, 0, 0, 1, 1, 0]`\n",
    "\n",
    "- Doc 3: `\"el perro duerme con el gato\"`  \n",
    "  - `el`: 2, `perro`: 1, `gato`: 1, `duerme`: 1, `con`: 1  \n",
    "  - Vector: `[2, 1, 0, 1, 1, 1]`\n",
    "\n",
    "Estos vectores suelen ser **largos y dispersos** (muchos ceros) cuando el vocabulario es grande.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2864dec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulario: ['con' 'duerme' 'el' 'gato' 'muerde' 'perro']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>con</th>\n",
       "      <th>duerme</th>\n",
       "      <th>el</th>\n",
       "      <th>gato</th>\n",
       "      <th>muerde</th>\n",
       "      <th>perro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Doc 1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc 2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc 3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       con  duerme  el  gato  muerde  perro\n",
       "Doc 1    0       0   1     0       1      1\n",
       "Doc 2    0       1   1     1       0      0\n",
       "Doc 3    1       1   2     1       0      1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Bag-of-Words con scikit-learn\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# Corpus de ejemplo (mismo que en la explicación)\n",
    "docs = [\n",
    "    \"el perro muerde\",\n",
    "    \"el gato duerme\",\n",
    "    \"el perro duerme con el gato\"\n",
    "]\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X_bow = vectorizer.fit_transform(docs)\n",
    "\n",
    "# Vocabulario aprendido\n",
    "vocab = vectorizer.get_feature_names_out()\n",
    "print(\"Vocabulario:\", vocab)\n",
    "\n",
    "# Representación BoW como DataFrame para verlo más claro\n",
    "df_bow = pd.DataFrame(X_bow.toarray(), columns=vocab)\n",
    "df_bow.index = [f\"Doc {i+1}\" for i in range(len(docs))]\n",
    "df_bow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0495df",
   "metadata": {},
   "source": [
    "## 2. TF-IDF (Term Frequency – Inverse Document Frequency)\n",
    "\n",
    "### 2.1. Motivación\n",
    "\n",
    "Bag-of-Words trata igual todas las palabras. Pero en la práctica:\n",
    "\n",
    "- Palabras como **\"el\", \"la\", \"de\"** no son informativas (aparecen en casi todos los documentos).  \n",
    "- Palabras como **\"inflación\", \"muerde\", \"con\"** pueden ser más informativas.\n",
    "\n",
    "TF-IDF ajusta los conteos para **dar más peso** a las palabras que:\n",
    "\n",
    "- Son **frecuentes en un documento**, pero  \n",
    "- **Raras en el corpus completo**.\n",
    "\n",
    "### 2.2. Definición\n",
    "\n",
    "Para una palabra \\(t\\), documento \\(d\\) y un corpus de \\(N\\) documentos:\n",
    "\n",
    "- **TF(t, d)**: frecuencia de la palabra *t* en el documento *d*.  \n",
    "- **DF(t)**: número de documentos en los que aparece *t*.  \n",
    "- **IDF(t)** = $ \\log \\left( \\frac{N}{DF(t)} \\right) $.  \n",
    "\n",
    "Luego:\n",
    "\n",
    "$$ \\text{TF-IDF}(t, d) = TF(t, d) \\times IDF(t) $$\n",
    "\n",
    "### 2.3. Ejemplo simplificado con nuestro corpus\n",
    "\n",
    "Tenemos 3 documentos (N = 3) y el mismo vocabulario:\n",
    "\n",
    "`[\"el\", \"perro\", \"muerde\", \"gato\", \"duerme\", \"con\"]`\n",
    "\n",
    "Digamos que:\n",
    "\n",
    "- `el` aparece en los 3 documentos → DF(el) = 3 → IDF(el) = log(3/3) = 0  \n",
    "\n",
    "- `muerde` aparece solo en 1 documento → DF(muerde) = 1 → IDF(muerde) = log(3/1) ≈ 1.10  \n",
    "\n",
    "- `con` aparece solo en 1 documento → IDF(con) ≈ 1.10  \n",
    "\n",
    "- `perro`, `gato`, `duerme` aparecen en 2 documentos → IDF ≈ log(3/2) ≈ 0.40  \n",
    "\n",
    "Conclusión:\n",
    "\n",
    "- Palabras muy frecuentes en el corpus (como \"el\") tienen **IDF bajo** → poca importancia.  \n",
    "- Palabras raras (como \"muerde\" o \"con\") tienen **IDF alto** → más importancia.\n",
    "\n",
    "El vector TF-IDF tiene la misma forma que BoW, pero en vez de conteos brutos, hay **pesos reales**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b2782b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulario: ['con' 'duerme' 'el' 'gato' 'muerde' 'perro']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>con</th>\n",
       "      <th>duerme</th>\n",
       "      <th>el</th>\n",
       "      <th>gato</th>\n",
       "      <th>muerde</th>\n",
       "      <th>perro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Doc 1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.425441</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.720333</td>\n",
       "      <td>0.547832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc 2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.619805</td>\n",
       "      <td>0.481334</td>\n",
       "      <td>0.619805</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc 3</th>\n",
       "      <td>0.492038</td>\n",
       "      <td>0.374207</td>\n",
       "      <td>0.581211</td>\n",
       "      <td>0.374207</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.374207</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            con    duerme        el      gato    muerde     perro\n",
       "Doc 1  0.000000  0.000000  0.425441  0.000000  0.720333  0.547832\n",
       "Doc 2  0.000000  0.619805  0.481334  0.619805  0.000000  0.000000\n",
       "Doc 3  0.492038  0.374207  0.581211  0.374207  0.000000  0.374207"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. TF-IDF con scikit-learn\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "docs = [\n",
    "    \"el perro muerde\",\n",
    "    \"el gato duerme\",\n",
    "    \"el perro duerme con el gato\"\n",
    "]\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "X_tfidf = tfidf.fit_transform(docs)\n",
    "\n",
    "vocab_tfidf = tfidf.get_feature_names_out()\n",
    "print(\"Vocabulario:\", vocab_tfidf)\n",
    "\n",
    "df_tfidf = pd.DataFrame(X_tfidf.toarray(), columns=vocab_tfidf)\n",
    "df_tfidf.index = [f\"Doc {i+1}\" for i in range(len(docs))]\n",
    "df_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147c226e",
   "metadata": {},
   "source": [
    "## 3. Embeddings y rol de Word2Vec\n",
    "\n",
    "### 3.1. ¿Qué es un embedding?\n",
    "\n",
    "En lugar de representar una palabra como:\n",
    "\n",
    "- Una posición en un vector gigante (BoW), o  \n",
    "- Un peso en un vector de TF-IDF,\n",
    "\n",
    "un **embedding** representa cada palabra como un **vector d-dimensional denso**, por ejemplo d = 100 o d = 300.\n",
    "\n",
    "Ejemplo (puramente ilustrativo, d = 3):\n",
    "\n",
    "- `perro` → [0.9, 0.1, 0.7]  \n",
    "- `gato` → [0.92, 0.15, 0.65]  \n",
    "- `muerde` → [0.4, 0.8, 0.1]  \n",
    "- `duerme` → [0.35, 0.75, 0.12]  \n",
    "\n",
    "Puntos clave:\n",
    "\n",
    "- Palabras **semánticamente similares** tienen vectores parecidos  \n",
    "  (por ejemplo, `perro` y `gato`).  \n",
    "- Los vectores son **densos y de baja dimensión** → más compactos que BoW/TF-IDF.\n",
    "\n",
    "### 3.2. ¿Cómo representar un documento con embeddings?\n",
    "\n",
    "Un documento es una secuencia de palabras. Si cada palabra tiene su embedding, se puede:\n",
    "\n",
    "- Hacer el **promedio** de los vectores de sus palabras.  \n",
    "- Hacer una **suma ponderada**.  \n",
    "- Usar modelos más complejos (RNN, Transformers) para producir un embedding de todo el documento.\n",
    "\n",
    "Ejemplo (promedio simple):\n",
    "\n",
    "Documento: `\"el perro muerde\"`  \n",
    "Supongamos embeddings (ficticios) para `el`, `perro`, `muerde`.  \n",
    "El embedding del documento = promedio de estos tres vectores.\n",
    "\n",
    "### 3.3. ¿Qué es Word2Vec y qué rol cumple?\n",
    "\n",
    "**Word2Vec no es el vector, es el modelo que aprende los vectores.**\n",
    "\n",
    "Word2Vec es un algoritmo de *deep learning* ligero que, dado un gran corpus de texto, aprende embeddings de palabras.\n",
    "\n",
    "Tiene dos variantes principales:\n",
    "\n",
    "- **CBOW (Continuous Bag-of-Words)**:  \n",
    "  Predice una palabra dado su contexto.\n",
    "\n",
    "- **Skip-gram**:  \n",
    "  Predice palabras de contexto dado una palabra central.\n",
    "\n",
    "Durante este proceso de predicción, el modelo ajusta los vectores de forma que:\n",
    "\n",
    "- Palabras que aparecen en contextos similares terminen con embeddings similares.\n",
    "\n",
    "### 3.4. Resumen del flujo con embeddings\n",
    "\n",
    "1. Entrenas (o descargas) un modelo de embeddings (por ejemplo, Word2Vec).  \n",
    "2. Obtienes un vector d-dimensional para cada palabra del vocabulario.  \n",
    "3. Representas cada documento combinando los vectores de sus palabras.  \n",
    "4. Usas esos vectores como entrada a un clasificador (regresión logística, SVM, red neuronal, etc.).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "987227cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding de 'perro':\n",
      "[-0.07511582 -0.00930042  0.09538119 -0.07319167 -0.02333769 -0.01937741\n",
      "  0.08077437 -0.05930896  0.00045162 -0.04753734]\n",
      "\n",
      "Similitud entre 'perro' y 'gato': -0.10551018\n",
      "\n",
      "Embedding del documento 'el perro muerde':\n",
      "[-0.05401909  0.01267396  0.03501464  0.00838214 -0.01046033 -0.04505576\n",
      "  0.0635127  -0.01248289 -0.02839585  0.00293801]\n"
     ]
    }
   ],
   "source": [
    "# 3. Ejemplo de código para entrenar Word2Vec con gensim\n",
    "# Nota: esto es un ejemplo; requiere tener instalado 'gensim'.\n",
    "\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Corpus de ejemplo tokenizado (lista de listas de palabras)\n",
    "sentences = [\n",
    "    [\"el\", \"perro\", \"muerde\"],\n",
    "    [\"el\", \"gato\", \"duerme\"],\n",
    "    [\"el\", \"perro\", \"duerme\", \"con\", \"el\", \"gato\"]\n",
    "]\n",
    "\n",
    "# Entrenar un modelo Word2Vec pequeño\n",
    "model = Word2Vec(\n",
    "    sentences=sentences,\n",
    "    vector_size=10,  # dimensión del embedding\n",
    "    window=2,        # tamaño de la ventana de contexto\n",
    "    min_count=1,     # incluir todas las palabras (por ser ejemplo pequeño)\n",
    "    workers=1,\n",
    "    sg=1             # 1 = Skip-gram, 0 = CBOW\n",
    ")\n",
    "\n",
    "# Obtener el embedding de una palabra\n",
    "print(\"Embedding de 'perro':\")\n",
    "print(model.wv[\"perro\"])\n",
    "\n",
    "# Calcular similitud entre palabras\n",
    "sim_perro_gato = model.wv.similarity(\"perro\", \"gato\")\n",
    "print(\"\\nSimilitud entre 'perro' y 'gato':\", sim_perro_gato)\n",
    "\n",
    "# Representar un documento como promedio de embeddings\n",
    "import numpy as np\n",
    "\n",
    "def document_embedding(tokens, wv):\n",
    "    vecs = [wv[w] for w in tokens if w in wv]\n",
    "    if not vecs:\n",
    "        return np.zeros(wv.vector_size)\n",
    "    return np.mean(vecs, axis=0)\n",
    "\n",
    "doc_tokens = [\"el\", \"perro\", \"muerde\"]\n",
    "doc_vec = document_embedding(doc_tokens, model.wv)\n",
    "print(\"\\nEmbedding del documento 'el perro muerde':\")\n",
    "print(doc_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c266bb59",
   "metadata": {},
   "source": [
    "# =============================================\n",
    "# \n",
    "# Cargar 1 PDF + 1 Word → Extraer Texto → TF-IDF\n",
    "# ============================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "265689f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Librerías cargadas correctamente.\n"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "from docx import Document\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "print(\"Librerías cargadas correctamente.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b827f612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------\n",
    "# RUTAS DE PRUEBA\n",
    "# -------------------------------------------------\n",
    "\n",
    "PDF_PATH = \"ejemplo.pdf\"      #\n",
    "WORD_PATH = \"ejemplo.docx\"    # \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca7c878b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------\n",
    "# Leer PDF (solo PDFs con texto digital)\n",
    "# -------------------------------------------------\n",
    "def leer_pdf_texto(path_pdf):\n",
    "    texto = \"\"\n",
    "    with pdfplumber.open(path_pdf) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            contenido = page.extract_text()\n",
    "            if contenido:\n",
    "                texto += contenido + \"\\n\"\n",
    "    return texto.strip()\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Leer Word (.docx)\n",
    "# -------------------------------------------------\n",
    "def leer_word(path_docx):\n",
    "    doc = Document(path_docx)\n",
    "    texto = \"\\n\".join([p.text for p in doc.paragraphs])\n",
    "    return texto.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99852870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF extraído:\n",
      "Documento de Ejemplo Extenso - PDF\n",
      "Este es un documento PDF más largo, creado para demostrar la extracción de texto desde\n",
      "archivos PDF que contienen texto nativo. Su objetivo es permitir que los estudiantes del curso\n",
      "verifiquen cómo los modelos de Machine Learning procesan documentos reales o simulados antes\n",
      "de convertirlos en representaciones numéricas como TF–IDF.\n",
      "El contenido incluye un análisis ficticio del comportamiento legislativo en una nación imaginaria,\n",
      "considerando estadísticas de proyectos de ley, indicadores de eficiencia parlamentaria y métricas\n",
      "de calidad regulatoria. Asimismo, se mencionan las dificultades comunes en la digitalización de\n",
      "documentos, la necesidad de sistemas OCR robustos y los desafíos técnicos asociados al\n",
      "procesamiento de archivos provenientes de múltiples formatos como CSV, PDF, Word y bases de\n",
      "datos externas.\n",
      "Se discuten también temas relevantes como el uso de modelos de clasificación para categorizar\n",
      "artículos de prensa, la construcción de sistemas RAG para respuestas contextualizadas, y la\n",
      "integración de herramientas de IA generativa en flujos de trabajo institucionales.\n",
      "El documento está diseñado intencionalmente con párrafos extensos para asegurar que los\n",
      "experimentos con TF–IDF produzcan vectores suficientemente densos y que los modelos puedan\n",
      "capturar patrones reales durante el entrenamiento. ...\n",
      "\n",
      "Word extraído:\n",
      "Documento de Ejemplo Extenso - Word\n",
      "Este es un documento de ejemplo más extenso, creado para probar la extracción de texto desde archivos .docx\n",
      "en el contexto del curso de Inteligencia Artificial aplicado a procesamiento de documentos.\n",
      "El objetivo es simular un documento técnico similar a los que maneja la Biblioteca del Congreso Nacional,\n",
      "incluyendo párrafos largos, ideas complejas y lenguaje más formal. El contenido no apunta a ningún tema real,\n",
      "pero replica la estructura de informes y minutas de asesoría parlamentaria.\n",
      "En este informe ficticio se analiza el comportamiento económico de una región hipotética durante los últimos\n",
      "cinco años, considerando variables como crecimiento, inflación, gasto público, inversión privada y desarrollo\n",
      "territorial. Asimismo, se revisan políticas públicas implementadas en áreas de salud, educación y transporte,\n",
      "junto con sus impactos esperados en la población.\n",
      "Finalmente, se presentan recomendaciones generales para la toma de decisiones estratégicas, incluyendo la \n",
      "necesidad de fortalecer los mecanismos de evaluación de políticas, mejorar la disponibilidad de datos y \n",
      "promover el uso de herramientas de análisis automatizado dentro de las unidades técnicas del Congreso.\n",
      "Este documento es solo un ejemplo para fines didácticos, pero incluye suficiente texto para probar TF–IDF, \n",
      "limpieza, segmentación y clasificación automática mediante modelos como Logistic Regression, Decision Tree \n",
      "o Random Forest. ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------\n",
    "# EXTRAER TEXTO DE LOS DOS DOCUMENTOS\n",
    "# -------------------------------------------------\n",
    "\n",
    "texto_pdf = leer_pdf_texto(PDF_PATH)\n",
    "texto_word = leer_word(WORD_PATH)\n",
    "\n",
    "print(\"PDF extraído:\")\n",
    "print(texto_pdf[:1500], \"...\\n\")\n",
    "\n",
    "print(\"Word extraído:\")\n",
    "print(texto_word[:1500], \"...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "23bdcb20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>texto</th>\n",
       "      <th>etiqueta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Documento de Ejemplo Extenso - PDF\\nEste es un...</td>\n",
       "      <td>pdf_demo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Documento de Ejemplo Extenso - Word\\nEste es u...</td>\n",
       "      <td>word_demo</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               texto   etiqueta\n",
       "0  Documento de Ejemplo Extenso - PDF\\nEste es un...   pdf_demo\n",
       "1  Documento de Ejemplo Extenso - Word\\nEste es u...  word_demo"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -------------------------------------------------\n",
    "# ARMAR DATASET\n",
    "# -------------------------------------------------\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"texto\": [texto_pdf, texto_word],\n",
    "    \"etiqueta\": [\"pdf_demo\", \"word_demo\"]   # etiquetas ejemplo\n",
    "})\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "35fdc254",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>texto</th>\n",
       "      <th>etiqueta</th>\n",
       "      <th>texto_limpio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Documento de Ejemplo Extenso - PDF\\nEste es un...</td>\n",
       "      <td>pdf_demo</td>\n",
       "      <td>documento de ejemplo extenso - pdf este es un ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Documento de Ejemplo Extenso - Word\\nEste es u...</td>\n",
       "      <td>word_demo</td>\n",
       "      <td>documento de ejemplo extenso - word este es un...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               texto   etiqueta  \\\n",
       "0  Documento de Ejemplo Extenso - PDF\\nEste es un...   pdf_demo   \n",
       "1  Documento de Ejemplo Extenso - Word\\nEste es u...  word_demo   \n",
       "\n",
       "                                        texto_limpio  \n",
       "0  documento de ejemplo extenso - pdf este es un ...  \n",
       "1  documento de ejemplo extenso - word este es un...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -------------------------------------------------\n",
    "# LIMPIEZA SUAVE\n",
    "# -------------------------------------------------\n",
    "\n",
    "def limpiar(texto):\n",
    "    texto = texto.lower().replace(\"\\n\", \" \")\n",
    "    return texto\n",
    "\n",
    "df[\"texto_limpio\"] = df[\"texto\"].apply(limpiar)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "deed159c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape matriz TF-IDF: (2, 430)\n"
     ]
    }
   ],
   "source": [
    "# Lista personalizada de stopwords en español\n",
    "spanish_stopwords = [\n",
    "    \"de\", \"la\", \"que\", \"el\", \"en\", \"y\", \"a\", \"los\", \"del\", \"se\", \"las\", \"por\", \"un\",\n",
    "    \"para\", \"con\", \"no\", \"una\", \"su\", \"al\", \"lo\", \"como\", \"más\", \"pero\", \"sus\", \"le\",\n",
    "    \"ya\", \"o\", \"este\", \"sí\", \"porque\", \"esta\", \"entre\", \"cuando\", \"muy\", \"sin\", \"sobre\",\n",
    "    \"también\", \"me\", \"hasta\", \"hay\", \"donde\", \"quien\", \"desde\", \"todo\", \"nos\", \"durante\",\n",
    "    \"todos\", \"uno\", \"les\", \"ni\", \"contra\", \"otros\", \"ese\", \"eso\", \"ante\", \"ellos\", \"e\", \n",
    "    \"esto\", \"mí\", \"antes\", \"algunos\", \"qué\", \"unos\", \"yo\", \"otro\", \"otras\", \"otra\",\n",
    "    \"él\", \"tanto\", \"esa\", \"estos\", \"mucho\", \"quienes\", \"nada\", \"muchos\", \"cual\", \"poco\",\n",
    "    \"ella\", \"estaré\"\n",
    "]\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    stop_words=spanish_stopwords,\n",
    "    ngram_range=(1, 2),     # unigrams + bigrams (muy útil en documentos legales)\n",
    "    max_features=3000       # tamaño razonable para demos\n",
    ")\n",
    "\n",
    "X = vectorizer.fit_transform(df[\"texto_limpio\"])\n",
    "print(\"Shape matriz TF-IDF:\", X.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "23c7e728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['analiza' 'analiza comportamiento' 'análisis' 'análisis automatizado'\n",
      " 'análisis ficticio' 'aplicado' 'aplicado procesamiento' 'apunta'\n",
      " 'apunta ningún' 'archivos' 'archivos docx' 'archivos pdf'\n",
      " 'archivos provenientes' 'artificial' 'artificial aplicado' 'artículos'\n",
      " 'artículos prensa' 'asegurar' 'asegurar experimentos' 'asesoría'\n",
      " 'asesoría parlamentaria' 'asimismo' 'asimismo mencionan'\n",
      " 'asimismo revisan' 'asociados' 'asociados procesamiento' 'automatizado'\n",
      " 'automatizado dentro' 'automática' 'automática mediante' 'años'\n",
      " 'años considerando' 'bases' 'bases datos' 'biblioteca'\n",
      " 'biblioteca congreso' 'calidad' 'calidad regulatoria' 'capturar'\n",
      " 'capturar patrones' 'categorizar' 'categorizar artículos' 'cinco'\n",
      " 'cinco años' 'clasificación' 'clasificación automática'\n",
      " 'clasificación categorizar' 'complejas' 'complejas lenguaje'\n",
      " 'comportamiento' 'comportamiento económico' 'comportamiento legislativo'\n",
      " 'comunes' 'comunes digitalización' 'congreso' 'congreso documento'\n",
      " 'congreso nacional' 'considerando' 'considerando estadísticas'\n",
      " 'considerando variables' 'construcción' 'construcción sistemas'\n",
      " 'contenido' 'contenido apunta' 'contenido incluye' 'contexto'\n",
      " 'contexto curso' 'contextualizadas' 'contextualizadas integración'\n",
      " 'contienen' 'contienen texto' 'convertirlos'\n",
      " 'convertirlos representaciones' 'creado' 'creado demostrar'\n",
      " 'creado probar' 'crecimiento' 'crecimiento inflación' 'csv' 'csv pdf'\n",
      " 'curso' 'curso inteligencia' 'curso verifiquen' 'cómo' 'cómo modelos'\n",
      " 'datos' 'datos externas' 'datos promover' 'decision' 'decision tree'\n",
      " 'decisiones' 'decisiones estratégicas' 'demostrar' 'demostrar extracción'\n",
      " 'densos' 'densos modelos' 'dentro' 'dentro unidades' 'desafíos'\n",
      " 'desafíos técnicos' 'desarrollo' 'desarrollo territorial' 'didácticos'\n",
      " 'didácticos incluye' 'dificultades' 'dificultades comunes'\n",
      " 'digitalización' 'digitalización documentos' 'discuten' 'discuten temas'\n",
      " 'diseñado' 'diseñado intencionalmente' 'disponibilidad'\n",
      " 'disponibilidad datos' 'documento' 'documento ejemplo' 'documento es'\n",
      " 'documento está' 'documento pdf' 'documento técnico' 'documentos'\n",
      " 'documentos necesidad' 'documentos objetivo' 'documentos reales' 'docx'\n",
      " 'docx contexto' 'económico' 'económico región' 'educación'\n",
      " 'educación transporte' 'eficiencia' 'eficiencia parlamentaria' 'ejemplo'\n",
      " 'ejemplo extenso' 'ejemplo fines' 'entrenamiento' 'es' 'es documento'\n",
      " 'es permitir' 'es simular' 'es solo' 'esperados' 'esperados población'\n",
      " 'estadísticas' 'estadísticas proyectos' 'estratégicas'\n",
      " 'estratégicas incluyendo' 'estructura' 'estructura informes'\n",
      " 'estudiantes' 'estudiantes curso' 'está' 'está diseñado' 'evaluación'\n",
      " 'evaluación políticas' 'experimentos' 'experimentos tf' 'extenso'\n",
      " 'extenso creado' 'extenso pdf' 'extenso word' 'extensos'\n",
      " 'extensos asegurar' 'externas' 'externas discuten' 'extracción'\n",
      " 'extracción texto' 'ficticio' 'ficticio analiza'\n",
      " 'ficticio comportamiento' 'finalmente' 'finalmente presentan' 'fines'\n",
      " 'fines didácticos' 'flujos' 'flujos trabajo' 'forest' 'formal'\n",
      " 'formal contenido' 'formatos' 'formatos csv' 'fortalecer'\n",
      " 'fortalecer mecanismos' 'gasto' 'gasto público' 'generales'\n",
      " 'generales toma' 'generativa' 'generativa flujos' 'herramientas'\n",
      " 'herramientas análisis' 'herramientas ia' 'hipotética'\n",
      " 'hipotética últimos' 'ia' 'ia generativa' 'ideas' 'ideas complejas' 'idf'\n",
      " 'idf contenido' 'idf limpieza' 'idf produzcan' 'imaginaria'\n",
      " 'imaginaria considerando' 'impactos' 'impactos esperados' 'implementadas'\n",
      " 'implementadas áreas' 'incluye' 'incluye análisis' 'incluye suficiente'\n",
      " 'incluyendo' 'incluyendo necesidad' 'incluyendo párrafos' 'indicadores'\n",
      " 'indicadores eficiencia' 'inflación' 'inflación gasto' 'informe'\n",
      " 'informe ficticio' 'informes' 'informes minutas' 'institucionales'\n",
      " 'institucionales documento' 'integración' 'integración herramientas'\n",
      " 'inteligencia' 'inteligencia artificial' 'intencionalmente'\n",
      " 'intencionalmente párrafos' 'inversión' 'inversión privada' 'junto'\n",
      " 'junto impactos' 'largo' 'largo creado' 'largos' 'largos ideas'\n",
      " 'learning' 'learning procesan' 'legislativo' 'legislativo nación'\n",
      " 'lenguaje' 'lenguaje formal' 'ley' 'ley indicadores' 'limpieza'\n",
      " 'limpieza segmentación' 'logistic' 'logistic regression' 'machine'\n",
      " 'machine learning' 'maneja' 'maneja biblioteca' 'mecanismos'\n",
      " 'mecanismos evaluación' 'mediante' 'mediante modelos' 'mejorar'\n",
      " 'mejorar disponibilidad' 'mencionan' 'mencionan dificultades' 'minutas'\n",
      " 'minutas asesoría' 'modelos' 'modelos clasificación' 'modelos logistic'\n",
      " 'modelos machine' 'modelos puedan' 'métricas' 'métricas calidad'\n",
      " 'múltiples' 'múltiples formatos' 'nacional' 'nacional incluyendo'\n",
      " 'nación' 'nación imaginaria' 'nativo' 'nativo objetivo' 'necesidad'\n",
      " 'necesidad fortalecer' 'necesidad sistemas' 'ningún' 'ningún tema'\n",
      " 'numéricas' 'numéricas tf' 'objetivo' 'objetivo es' 'ocr' 'ocr robustos'\n",
      " 'parlamentaria' 'parlamentaria informe' 'parlamentaria métricas'\n",
      " 'patrones' 'patrones reales' 'pdf' 'pdf contienen' 'pdf es' 'pdf largo'\n",
      " 'pdf word' 'permitir' 'permitir estudiantes' 'población'\n",
      " 'población finalmente' 'políticas' 'políticas mejorar'\n",
      " 'políticas públicas' 'prensa' 'prensa construcción' 'presentan'\n",
      " 'presentan recomendaciones' 'privada' 'privada desarrollo' 'probar'\n",
      " 'probar extracción' 'probar tf' 'procesamiento' 'procesamiento archivos'\n",
      " 'procesamiento documentos' 'procesan' 'procesan documentos' 'produzcan'\n",
      " 'produzcan vectores' 'promover' 'promover uso' 'provenientes'\n",
      " 'provenientes múltiples' 'proyectos' 'proyectos ley' 'puedan'\n",
      " 'puedan capturar' 'párrafos' 'párrafos extensos' 'párrafos largos'\n",
      " 'públicas' 'públicas implementadas' 'público' 'público inversión' 'rag'\n",
      " 'rag respuestas' 'random' 'random forest' 'real' 'real replica' 'reales'\n",
      " 'reales entrenamiento' 'reales simulados' 'recomendaciones'\n",
      " 'recomendaciones generales' 'región' 'región hipotética' 'regression'\n",
      " 'regression decision' 'regulatoria' 'regulatoria asimismo' 'relevantes'\n",
      " 'relevantes uso' 'replica' 'replica estructura' 'representaciones'\n",
      " 'representaciones numéricas' 'respuestas' 'respuestas contextualizadas'\n",
      " 'revisan' 'revisan políticas' 'robustos' 'robustos desafíos' 'salud'\n",
      " 'salud educación' 'segmentación' 'segmentación clasificación' 'similar'\n",
      " 'similar maneja' 'simulados' 'simulados convertirlos' 'simular'\n",
      " 'simular documento' 'sistemas' 'sistemas ocr' 'sistemas rag' 'solo'\n",
      " 'solo ejemplo' 'suficiente' 'suficiente texto' 'suficientemente'\n",
      " 'suficientemente densos' 'tema' 'tema real' 'temas' 'temas relevantes'\n",
      " 'territorial' 'territorial asimismo' 'texto' 'texto archivos'\n",
      " 'texto nativo' 'texto probar' 'tf' 'tf idf' 'toma' 'toma decisiones'\n",
      " 'trabajo' 'trabajo institucionales' 'transporte' 'transporte junto'\n",
      " 'tree' 'tree random' 'técnicas' 'técnicas congreso' 'técnico'\n",
      " 'técnico similar' 'técnicos' 'técnicos asociados' 'unidades'\n",
      " 'unidades técnicas' 'uso' 'uso herramientas' 'uso modelos' 'variables'\n",
      " 'variables crecimiento' 'vectores' 'vectores suficientemente'\n",
      " 'verifiquen' 'verifiquen cómo' 'word' 'word bases' 'word es' 'áreas'\n",
      " 'áreas salud' 'últimos' 'últimos cinco']\n"
     ]
    }
   ],
   "source": [
    "feature_names = vectorizer.get_feature_names_out()\n",
    "print(feature_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d9cbff59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>analiza</th>\n",
       "      <th>analiza comportamiento</th>\n",
       "      <th>análisis</th>\n",
       "      <th>análisis automatizado</th>\n",
       "      <th>análisis ficticio</th>\n",
       "      <th>aplicado</th>\n",
       "      <th>aplicado procesamiento</th>\n",
       "      <th>apunta</th>\n",
       "      <th>apunta ningún</th>\n",
       "      <th>archivos</th>\n",
       "      <th>...</th>\n",
       "      <th>vectores suficientemente</th>\n",
       "      <th>verifiquen</th>\n",
       "      <th>verifiquen cómo</th>\n",
       "      <th>word</th>\n",
       "      <th>word bases</th>\n",
       "      <th>word es</th>\n",
       "      <th>áreas</th>\n",
       "      <th>áreas salud</th>\n",
       "      <th>últimos</th>\n",
       "      <th>últimos cinco</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.045694</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.064221</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.091388</td>\n",
       "      <td>...</td>\n",
       "      <td>0.064221</td>\n",
       "      <td>0.064221</td>\n",
       "      <td>0.064221</td>\n",
       "      <td>0.045694</td>\n",
       "      <td>0.064221</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.061839</td>\n",
       "      <td>0.061839</td>\n",
       "      <td>0.043999</td>\n",
       "      <td>0.061839</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.061839</td>\n",
       "      <td>0.061839</td>\n",
       "      <td>0.061839</td>\n",
       "      <td>0.061839</td>\n",
       "      <td>0.043999</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.043999</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.061839</td>\n",
       "      <td>0.061839</td>\n",
       "      <td>0.061839</td>\n",
       "      <td>0.061839</td>\n",
       "      <td>0.061839</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 430 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    analiza  analiza comportamiento  análisis  análisis automatizado  \\\n",
       "0  0.000000                0.000000  0.045694               0.000000   \n",
       "1  0.061839                0.061839  0.043999               0.061839   \n",
       "\n",
       "   análisis ficticio  aplicado  aplicado procesamiento    apunta  \\\n",
       "0           0.064221  0.000000                0.000000  0.000000   \n",
       "1           0.000000  0.061839                0.061839  0.061839   \n",
       "\n",
       "   apunta ningún  archivos  ...  vectores suficientemente  verifiquen  \\\n",
       "0       0.000000  0.091388  ...                  0.064221    0.064221   \n",
       "1       0.061839  0.043999  ...                  0.000000    0.000000   \n",
       "\n",
       "   verifiquen cómo      word  word bases   word es     áreas  áreas salud  \\\n",
       "0         0.064221  0.045694    0.064221  0.000000  0.000000     0.000000   \n",
       "1         0.000000  0.043999    0.000000  0.061839  0.061839     0.061839   \n",
       "\n",
       "    últimos  últimos cinco  \n",
       "0  0.000000       0.000000  \n",
       "1  0.061839       0.061839  \n",
       "\n",
       "[2 rows x 430 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_tfidf = pd.DataFrame(X.toarray(), columns=feature_names)\n",
    "df_tfidf.head()   # primeras 5 filas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e40a6d7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicción del PDF: pdf_demo\n",
      "Predicción del Word: word_demo\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X, df[\"etiqueta\"])\n",
    "\n",
    "print(\"Predicción del PDF:\", clf.predict(X)[0])\n",
    "print(\"Predicción del Word:\", clf.predict(X)[1])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
