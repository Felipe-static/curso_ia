{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Chatbot Multi-PDF con Memoria Conversacional (Estilo Nativo)\n",
                "\n",
                "Este notebook demuestra cómo construir un sistema RAG (Retrieval-Augmented Generation) **sin depender de integraciones complejas de LangChain para la conexión con Google**, usando directamente las librerías oficiales.\n",
                "\n",
                "**Flujo de trabajo:**\n",
                "1.  **Ingesta**: Cargar PDFs y dividirlos en fragmentos (chunks).\n",
                "2.  **Vector Store Manual**: Usar `ChromaDB` nativo para guardar embeddings generados por `google-generativeai`.\n",
                "3.  **Memoria**: Gestión manual del historial de chat.\n",
                "4.  **RAG**: Buscar, construir prompt y generar respuesta.\n",
                "\n",
                "\n",
                "Recuerda que puedes obtener tu API Key en: https://aistudio.google.com/api-keys"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Instalación de librerías\n",
                "%pip install -q -U google-generativeai chromadb pypdf langchain_community langchain"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import google.generativeai as genai\n",
                "import chromadb\n",
                "from chromadb.config import Settings\n",
                "\n",
                "# Configura tu API Key aquí\n",
                "GOOGLE_API_KEY = \"TU API KEY\" \n",
                "genai.configure(api_key=GOOGLE_API_KEY)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Ingesta de Datos\n",
                "Usamos LangChain SOLO para cargar y dividir el texto, que es donde brilla."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
                "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
                "\n",
                "# 1. Cargar PDFs\n",
                "source_folder = \"libros\"\n",
                "\n",
                "# Crea la carpeta si no existe\n",
                "if not os.path.exists(source_folder):\n",
                "    os.makedirs(source_folder)\n",
                "    print(f\"La carpeta '{source_folder}' no existía. Se ha creado. ¡Pon tus PDFs ahí!\")\n",
                "\n",
                "loader = PyPDFDirectoryLoader(source_folder)\n",
                "documents = loader.load()\n",
                "\n",
                "# 2. Dividir (Chunking)\n",
                "text_splitter = RecursiveCharacterTextSplitter(\n",
                "    chunk_size=1000,\n",
                "    chunk_overlap=100\n",
                ")\n",
                "chunks_objects = text_splitter.split_documents(documents)\n",
                "\n",
                "# Convertimos a lista de textos simples para procesarlos manualmente\n",
                "chunks_text = [doc.page_content for doc in chunks_objects]\n",
                "\n",
                "print(f\"Encontrados {len(documents)} documentos. Creados {len(chunks_text)} fragmentos (chunks).\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Vector Store Manual (ChromaDB + Google Embeddings)\n",
                "En lugar de usar la capa de abstracción de LangChain, haremos el ciclo de embeddings \"a mano\" como en el ejemplo de YouTube."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Inicializamos Cliente Persistente de Chroma\n",
                "# Esto guardará la base de datos en una carpeta local para no re-procesar todo siempre\n",
                "chroma_client = chromadb.PersistentClient(path=\"./chroma_db_multi_pdf\")\n",
                "collection_name = \"pdf_knowledge_base\"\n",
                "\n",
                "# Borramos la colección anterior si queremos reiniciar limpios (opcional, útil para pruebas)\n",
                "try:\n",
                "    chroma_client.delete_collection(name=collection_name)\n",
                "except:\n",
                "    pass\n",
                "\n",
                "collection = chroma_client.create_collection(name=collection_name)\n",
                "\n",
                "print(\"Generando embeddings y guardando en ChromaDB...\")\n",
                "\n",
                "# Preparamos listas para el 'batch add' de Chroma\n",
                "ids = []\n",
                "embeddings = []\n",
                "metadatas = []\n",
                "docs_to_save = []\n",
                "\n",
                "# Procesamos cada chunk\n",
                "for i, text in enumerate(chunks_text):\n",
                "    try:\n",
                "        # Generamos embedding con la librería oficial de Google\n",
                "        res = genai.embed_content(\n",
                "            model=\"models/text-embedding-004\",\n",
                "            content=text,\n",
                "            task_type=\"retrieval_document\"\n",
                "        )\n",
                "        \n",
                "        ids.append(str(i))\n",
                "        embeddings.append(res['embedding'])\n",
                "        metadatas.append({\"source\": \"pdf_chunk\"}) # Puedes agregar más info aquí si quieres\n",
                "        docs_to_save.append(text)\n",
                "        \n",
                "    except Exception as e:\n",
                "        print(f\"Error generando embedding para chunk {i}: {e}\")\n",
                "\n",
                "# Guardamos todo en Chroma\n",
                "if ids:\n",
                "    collection.add(\n",
                "        documents=docs_to_save,\n",
                "        embeddings=embeddings,\n",
                "        metadatas=metadatas,\n",
                "        ids=ids\n",
                "    )\n",
                "    print(f\"Guardados {len(ids)} chunks en la Vector Store.\")\n",
                "else:\n",
                "    print(\"No se guardó nada (revisa si hay pdf's).\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Sistema de Chat (Memoria + RAG)\n",
                "Ahora configuramos el bucle de chat."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Configuración del modelo\n",
                "model = genai.GenerativeModel('models/gemini-2.5-flash')\n",
                "chat_history = []\n",
                "\n",
                "def format_history(history, limit=3):\n",
                "    # Tomamos los últimos 'limit' intercambios\n",
                "    recent = history[-(limit*2):]\n",
                "    formatted = \"\"\n",
                "    for i in range(0, len(recent), 2):\n",
                "        if i+1 < len(recent):\n",
                "            formatted += f\"Usuario: {recent[i]}\\nIA: {recent[i+1]}\\n\"\n",
                "    return formatted\n",
                "\n",
                "print(\"\\n========================================\")\n",
                "print(\"¡Chat Multi-PDF listo! Escribe 'exit' para terminar.\")\n",
                "print(\"========================================\\n\")\n",
                "\n",
                "while True:\n",
                "    question = input(\"Tú: \")\n",
                "    if question.lower() in [\"salir\", \"exit\", \"quit\"]:\n",
                "        print(\"Hasta la próxima!\")\n",
                "        break\n",
                "        \n",
                "    # 1. Recuperación (Retrieval)\n",
                "    try:\n",
                "        # Embed de la pregunta\n",
                "        q_emb = genai.embed_content(\n",
                "            model=\"models/text-embedding-004\",\n",
                "            content=question,\n",
                "            task_type=\"retrieval_query\"\n",
                "        )['embedding']\n",
                "        \n",
                "        # Query a chroma\n",
                "        results = collection.query(\n",
                "            query_embeddings=[q_emb],\n",
                "            n_results=3\n",
                "        )\n",
                "        \n",
                "        # Extraer texto de los resultados\n",
                "        retrieved_docs = results['documents'][0]\n",
                "        context_text = \"\\n---\\n\".join(retrieved_docs)\n",
                "        \n",
                "        # 2. Construcción del Prompt\n",
                "        history_txt = format_history(chat_history)\n",
                "        \n",
                "        prompt = f\"\"\"\n",
                "        Eres un asistente útil que responde preguntas sobre documentos PDF proporcionados.\n",
                "        Usa el siguiente contexto y el historial de chat para responder.\n",
                "\n",
                "        [HISTORIAL DE CHAT]\n",
                "        {history_txt}\n",
                "\n",
                "        [CONTEXTO RECUPERADO DE LOS PDFs]\n",
                "        {context_text}\n",
                "\n",
                "        [PREGUNTA ACTUAL]\n",
                "        {question}\n",
                "        \n",
                "        INSTRUCCIÓN: Responde solo basándote en el contexto. Si no sabes, dilo. Nunca debes alucinar respuestas.\n",
                "        \"\"\"\n",
                "        \n",
                "        # 3. Generación\n",
                "        response = model.generate_content(prompt)\n",
                "        answer = response.text\n",
                "        \n",
                "        print(f\"Usuario: {question}\")\n",
                "        print(f\"\\n Modelo RAG: {answer}\\n\")\n",
                "        \n",
                "        # 4. Actualizar Memoria\n",
                "        chat_history.append(question)\n",
                "        chat_history.append(answer)\n",
                "\n",
                "    except Exception as e:\n",
                "        print(f\"Error: {e}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
