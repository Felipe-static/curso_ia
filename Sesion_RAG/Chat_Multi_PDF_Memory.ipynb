{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Chatbot Multi-PDF con Memoria Conversacional (Estilo Nativo)\n",
                "\n",
                "Este notebook demuestra cómo construir un sistema RAG (Retrieval-Augmented Generation) **sin depender de integraciones complejas de LangChain para la conexión con Google**, usando directamente las librerías oficiales.\n",
                "\n",
                "**Flujo de trabajo:**\n",
                "1.  **Ingesta**: Cargar PDFs y dividirlos en fragmentos (chunks).\n",
                "2.  **Vector Store Manual**: Usar `ChromaDB` nativo para guardar embeddings generados por `google-generativeai`.\n",
                "3.  **Memoria**: Gestión manual del historial de chat.\n",
                "4.  **RAG**: Buscar, construir prompt y generar respuesta.\n",
                "\n",
                "\n",
                "Recuerda que puedes obtener tu API Key en: https://aistudio.google.com/api-keys"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Instalación de librerías\n",
                "%pip install -q -U google-generativeai chromadb pypdf langchain_community langchain"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "c:\\Users\\felip\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
                        "  from .autonotebook import tqdm as notebook_tqdm\n",
                        "C:\\Users\\felip\\AppData\\Local\\Temp\\ipykernel_5012\\2154271558.py:2: FutureWarning: \n",
                        "\n",
                        "All support for the `google.generativeai` package has ended. It will no longer be receiving \n",
                        "updates or bug fixes. Please switch to the `google.genai` package as soon as possible.\n",
                        "See README for more details:\n",
                        "\n",
                        "https://github.com/google-gemini/deprecated-generative-ai-python/blob/main/README.md\n",
                        "\n",
                        "  import google.generativeai as genai\n"
                    ]
                }
            ],
            "source": [
                "import os\n",
                "import google.generativeai as genai\n",
                "import chromadb\n",
                "from chromadb.config import Settings\n",
                "\n",
                "# Configura tu API Key aquí\n",
                "GOOGLE_API_KEY = \"TU API KEY\" \n",
                "genai.configure(api_key=GOOGLE_API_KEY)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Ingesta de Datos\n",
                "Usamos LangChain SOLO para cargar y dividir el texto, que es donde brilla."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Encontrados 263 documentos. Creados 671 fragmentos (chunks).\n"
                    ]
                }
            ],
            "source": [
                "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
                "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
                "\n",
                "# 1. Cargar PDFs\n",
                "source_folder = \"libros\"\n",
                "\n",
                "# Crea la carpeta si no existe\n",
                "if not os.path.exists(source_folder):\n",
                "    os.makedirs(source_folder)\n",
                "    print(f\"La carpeta '{source_folder}' no existía. Se ha creado. ¡Pon tus PDFs ahí!\")\n",
                "\n",
                "loader = PyPDFDirectoryLoader(source_folder)\n",
                "documents = loader.load()\n",
                "\n",
                "# 2. Dividir (Chunking)\n",
                "text_splitter = RecursiveCharacterTextSplitter(\n",
                "    chunk_size=1000,\n",
                "    chunk_overlap=100\n",
                ")\n",
                "chunks_objects = text_splitter.split_documents(documents)\n",
                "\n",
                "# Convertimos a lista de textos simples para procesarlos manualmente\n",
                "chunks_text = [doc.page_content for doc in chunks_objects]\n",
                "\n",
                "print(f\"Encontrados {len(documents)} documentos. Creados {len(chunks_text)} fragmentos (chunks).\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Vector Store Manual (ChromaDB + Google Embeddings)\n",
                "En lugar de usar la capa de abstracción de LangChain, haremos el ciclo de embeddings \"a mano\" como en el ejemplo de YouTube."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Generando embeddings y guardando en ChromaDB...\n",
                        "Guardados 671 chunks en la Vector Store.\n"
                    ]
                }
            ],
            "source": [
                "# Inicializamos Cliente Persistente de Chroma\n",
                "# Esto guardará la base de datos en una carpeta local para no re-procesar todo siempre\n",
                "chroma_client = chromadb.PersistentClient(path=\"./chroma_db_multi_pdf\")\n",
                "collection_name = \"pdf_knowledge_base\"\n",
                "\n",
                "# Borramos la colección anterior si queremos reiniciar limpios (opcional, útil para pruebas)\n",
                "try:\n",
                "    chroma_client.delete_collection(name=collection_name)\n",
                "except:\n",
                "    pass\n",
                "\n",
                "collection = chroma_client.create_collection(name=collection_name)\n",
                "\n",
                "print(\"Generando embeddings y guardando en ChromaDB...\")\n",
                "\n",
                "# Preparamos listas para el 'batch add' de Chroma\n",
                "ids = []\n",
                "embeddings = []\n",
                "metadatas = []\n",
                "docs_to_save = []\n",
                "\n",
                "# Procesamos cada chunk\n",
                "for i, text in enumerate(chunks_text):\n",
                "    try:\n",
                "        # Generamos embedding con la librería oficial de Google\n",
                "        res = genai.embed_content(\n",
                "            model=\"models/text-embedding-004\",\n",
                "            content=text,\n",
                "            task_type=\"retrieval_document\"\n",
                "        )\n",
                "        \n",
                "        ids.append(str(i))\n",
                "        embeddings.append(res['embedding'])\n",
                "        metadatas.append({\"source\": \"pdf_chunk\"}) # Puedes agregar más info aquí si quieres\n",
                "        docs_to_save.append(text)\n",
                "        \n",
                "    except Exception as e:\n",
                "        print(f\"Error generando embedding para chunk {i}: {e}\")\n",
                "\n",
                "# Guardamos todo en Chroma\n",
                "if ids:\n",
                "    collection.add(\n",
                "        documents=docs_to_save,\n",
                "        embeddings=embeddings,\n",
                "        metadatas=metadatas,\n",
                "        ids=ids\n",
                "    )\n",
                "    print(f\"Guardados {len(ids)} chunks en la Vector Store.\")\n",
                "else:\n",
                "    print(\"No se guardó nada (revisa si hay pdf's).\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Sistema de Chat (Memoria + RAG)\n",
                "Ahora configuramos el bucle de chat."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "========================================\n",
                        "¡Chat Multi-PDF listo! Escribe 'exit' para terminar.\n",
                        "========================================\n",
                        "\n",
                        "Usuario: Qué hace Marco Aurelio en su primer capitulo? Son los agradecimientos?\n",
                        "\n",
                        "\n",
                        " Modelo RAG: En su primer capítulo (LIBRO I), Marco Aurelio rememora los rasgos y cualidades más amables de las personas que influyeron en su vida, como su abuelo Vero (de quien aprendió el buen carácter y la serenidad), su progenitor (el carácter discreto y viril), y su madre (el respeto a los dioses, la generosidad y la abstención de obrar mal).\n",
                        "\n",
                        "Sí, el contexto indica que al hacer esto, Marco Aurelio pone de manifiesto su afecto hacia ellos, pagando así \"una cordial deuda de gratitud\", lo cual se asemeja a un acto de agradecimiento o reconocimientos.\n",
                        "\n",
                        "Usuario: Y que es lo que dice sobre su padre?\n",
                        "\n",
                        "\n",
                        " Modelo RAG: Sobre su padre, el narrador dice que no le da \"nada de pena, porque es tan injusto\". También menciona que su padre lo castigaría otra vez si volviera a casa, lo cual le quita las ganas de regresar. Además, cuenta que su mamá se queja porque el papá \"vuelve a lo del señor Ruletero\", lo que provoca peleas entre ellos.\n",
                        "\n",
                        "Usuario: Cual es la primera frase de papelucho? Y que hace Marco Aurelio que lo hace tan importante para la historia?\n",
                        "\n",
                        "\n",
                        " Modelo RAG: No se menciona la primera frase de Papelucho en el contexto proporcionado.\n",
                        "\n",
                        "Marco Aurelio es importante para la historia por su gran personalidad. Se le considera un tipo de héroe muy poco frecuente en la Historia, y la conexión de su posición histórica, su conducta personal y su actitud filosófica lo convierten en una figura atractiva y un ejemplo apasionante de humanidad.\n",
                        "\n",
                        "Usuario: Estas entendiendo cuando te hago 2 preguntas a la vez?\n",
                        "\n",
                        "\n",
                        " Modelo RAG: No hay información en el contexto proporcionado que me permita responder si entiendo cuando me haces dos preguntas a la vez.\n",
                        "\n",
                        "Usuario: cuantas lunas tiene la tierra?\n",
                        "\n",
                        "\n",
                        " Modelo RAG: No se menciona cuántas lunas tiene la Tierra en el contexto proporcionado.\n",
                        "\n",
                        "Usuario: que valora Marco Aurelio de su abuelo Vero? Cuando llega papelucho a viña del mar?\n",
                        "\n",
                        "\n",
                        " Modelo RAG: Marco Aurelio valora de su abuelo Vero \"el buen carácter y la serenidad\".\n",
                        "\n",
                        "No se menciona cuándo llega Papelucho a Viña del Mar en el contexto proporcionado.\n",
                        "\n",
                        "Usuario: Qué dia llega papelucho a viña del mar?\n",
                        "\n",
                        "\n",
                        " Modelo RAG: Papelucho llega a Viña del Mar el 11 de enero.\n",
                        "\n",
                        "Hasta la próxima!\n"
                    ]
                }
            ],
            "source": [
                "# Configuración del modelo\n",
                "model = genai.GenerativeModel('models/gemini-2.5-flash')\n",
                "chat_history = []\n",
                "\n",
                "def format_history(history, limit=3):\n",
                "    # Tomamos los últimos 'limit' intercambios\n",
                "    recent = history[-(limit*2):]\n",
                "    formatted = \"\"\n",
                "    for i in range(0, len(recent), 2):\n",
                "        if i+1 < len(recent):\n",
                "            formatted += f\"Usuario: {recent[i]}\\nIA: {recent[i+1]}\\n\"\n",
                "    return formatted\n",
                "\n",
                "print(\"\\n========================================\")\n",
                "print(\"¡Chat Multi-PDF listo! Escribe 'exit' para terminar.\")\n",
                "print(\"========================================\\n\")\n",
                "\n",
                "while True:\n",
                "    question = input(\"Tú: \")\n",
                "    if question.lower() in [\"salir\", \"exit\", \"quit\"]:\n",
                "        print(\"Hasta la próxima!\")\n",
                "        break\n",
                "        \n",
                "    # 1. Recuperación (Retrieval)\n",
                "    try:\n",
                "        # Embed de la pregunta\n",
                "        q_emb = genai.embed_content(\n",
                "            model=\"models/text-embedding-004\",\n",
                "            content=question,\n",
                "            task_type=\"retrieval_query\"\n",
                "        )['embedding']\n",
                "        \n",
                "        # Query a chroma\n",
                "        results = collection.query(\n",
                "            query_embeddings=[q_emb],\n",
                "            n_results=3\n",
                "        )\n",
                "        \n",
                "        # Extraer texto de los resultados\n",
                "        retrieved_docs = results['documents'][0]\n",
                "        context_text = \"\\n---\\n\".join(retrieved_docs)\n",
                "        \n",
                "        # 2. Construcción del Prompt\n",
                "        history_txt = format_history(chat_history)\n",
                "        \n",
                "        prompt = f\"\"\"\n",
                "        Eres un asistente útil que responde preguntas sobre documentos PDF proporcionados.\n",
                "        Usa el siguiente contexto y el historial de chat para responder.\n",
                "\n",
                "        [HISTORIAL DE CHAT]\n",
                "        {history_txt}\n",
                "\n",
                "        [CONTEXTO RECUPERADO DE LOS PDFs]\n",
                "        {context_text}\n",
                "\n",
                "        [PREGUNTA ACTUAL]\n",
                "        {question}\n",
                "        \n",
                "        INSTRUCCIÓN: Responde solo basándote en el contexto. Si no sabes, dilo. Nunca debes alucinar respuestas.\n",
                "        \"\"\"\n",
                "        \n",
                "        # 3. Generación\n",
                "        response = model.generate_content(prompt)\n",
                "        answer = response.text\n",
                "        \n",
                "        print(f\"Usuario: {question}\")\n",
                "        print(f\"\\n Modelo RAG: {answer}\\n\")\n",
                "        \n",
                "        # 4. Actualizar Memoria\n",
                "        chat_history.append(question)\n",
                "        chat_history.append(answer)\n",
                "\n",
                "    except Exception as e:\n",
                "        print(f\"Error: {e}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
